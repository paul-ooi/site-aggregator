---
title: Guardrails for the Algorithm: AI Governance
description: I recently attended Digital Colloquium's Accessibility Summit—hands down one of my favorite events of the year.
url: https://dubbot.com/dubblog/2025/guardrails-for-the-algorithm-ai-governance.html
source: https://dubbot.com/dubblog/index.html
organization: https://dubbot.com/dubblog/index.html
repostedDate: 2026-02-04T06:45:02.011Z
tags:
  - technology
  - accessibility
sourcePublishDate: 2025-08-07T00:00:00.000Z
author: Maggie Vaughan, CPACC
contentHash: 32798ab3412c4ba18c2bebe08931c496
---

I recently attended [Digital Colloquium's Accessibility Summit](https://events.digicol.org/a11ysummit25 "This link leaves the DubBlog website.")—hands down one of my favorite events of the year.

[Niki Ramesh](https://www.linkedin.com/in/nikitharamesh/ "This link leaves the DubBlog website.") delivered this year's keynote, **Using AI to Improve User Experience**, which was both timely and thought-provoking. 

As someone genuinely excited about the promise of AI—especially in the accessibility space—I find the possibilities for good to be boundless: more innovative tools, better experiences, more equitable digital access.

But with all that promise comes real risk: bias and discrimination baked into training data, the spread of AI-generated misinformation, and data privacy trade-offs.

Ms. Ramesh thoughtfully addressed these very risks and concerns in her keynote. One slide in particular resonated with me and served as the inspiration for this blog post.

> **_Are you asking the hard questions?_**
> 
> When developing or procuring AI services:
> 
> -   Is the data for AI fair and won’t harm people with disabilities?
> -   Are there partnerships in place to enrich data with diverse inputs?
> -   How can human oversight and verification be a part of the AI's operation?
> -   What safeguards prevent the generation of harmful content?
> 
> ~ **_Using AI to Improve User Experience_**, Niki Ramesh, Accessibility Summit 2025, Digital Colloquium

How do we begin to address these questions? Is there a system or process in place that can help ensure we are creating, providing, and using AI tools responsibly and ethically?

When we talk about AI, responsibility and ethics go hand in hand. Both are about looking closely at how AI systems are built and used—and spotting any ethical blind spots that could lead to unintended consequences. While many organizations are already navigating the practical side of responsible AI in their day-to-day operations, having a solid grasp of the bigger ethical picture can help lead to more strategic, future-focused choices.

## Enter AI Governance

> Artificial intelligence (AI) governance refers to the processes, standards, and guardrails that help ensure AI systems and tools are safe and ethical. 
> 
> ~ [What is AI governance](https://www.ibm.com/think/topics/ai-governance "This link leaves the DubBlog website.")? - IBM 

Without proper human oversight, AI can cause significant social and ethical harm. For instance, [Microsoft’s Tay Chatbot](https://en.wikipedia.org/wiki/Tay_\(chatbot\) "This link leaves the DubBlog website."), an AI designed to learn and interact with Twitter users, was launched to engage in online conversations. However, it quickly began repeating racist and offensive messages it learned from the platform, resulting in its shutdown within a day.

Similarly, the English tutoring company iTutor Group Inc. [found itself in legal hot water](https://www.eeoc.gov/newsroom/itutorgroup-pay-365000-settle-eeoc-discriminatory-hiring-suit "This link leaves the DubBlog website.") after deploying an AI-powered hiring tool that automatically screened out older job candidates. The algorithm was designed to reject female applicants over the age of 55 and male applicants over 60—regardless of their skills, experience, or qualifications.

These examples underscore the importance of [establishing and adhering to guidelines and frameworks](https://professional.dce.harvard.edu/blog/building-a-responsible-ai-framework-5-key-principles-for-organizations/ "This link leaves the DubBlog website.") that are crucial for ensuring AI systems do not infringe upon human dignity, rights, or safety.

At the core of responsible AI governance are five principles: empathy, bias control, transparency, and accountability.

[Michael Impink](https://professional.dce.harvard.edu/faculty/stephen-michael-impink/ "This link leaves the DubBlog website."), an instructor who teaches **_AI Ethics in Business_** at the Harvard Division of Continuing Education’s Professional and Executive Development, might suggest adding fairness, privacy, and security, [three of the five principles for ethical AI](https://professional.dce.harvard.edu/blog/building-a-responsible-ai-framework-5-key-principles-for-organizations/#What-Are-the-5-Key-Principles-of-Ethical-AI-for-Organizations "This link leaves the DubBlog website.").

To build and deploy AI responsibly, organizations must look beyond the technological innovation and bottom-line benefits and consider AI’s broader societal impact. This means anticipating how AI affects all stakeholders—not just users, but employees, communities, and society at large. A key part of this responsibility is human oversight and thorough examination and scrutiny of the training data to avoid embedding existing real-world biases into algorithms, which can reinforce unfairness and inequality, and erode trust. Transparency is also non-negotiable; organizations must be able to clearly explain how their AI systems make decisions, what logic underpins those outcomes, and how secure the data is. Ultimately, responsible AI means setting high standards early, committing to them consistently, and staying accountable as AI technologies evolve and scale.

## Enforcement

Much like [a sustainable, continuous accessibility monitoring strategy](https://dubbot.com/dubblog/2025/a-guide-to-continuous-accessibility-monitoring.html "This link leaves the DubBlog website."), AI governance requires constant oversight to make sure AI systems stay in line with evolving laws, ethical standards, and public expectations. 

It also takes input from across the board: AI developers, users, policymakers, and ethicists, ensuring that AI-related systems are built in ways that reflect shared societal values.

No matter the structure of your AI governance team—whether it’s a formal committee, an internal task force, or a multi-stakeholder advisory group—it must be empowered to:

-   Develop and enforce clear, actionable guidelines for AI development and deployment
-   Create a consistent framework for navigating ethical gray areas
-   Regularly review and revise those guidelines to keep up with fast-moving AI tech
-   Assign clear ownership for each stage of the AI lifecycle

[Building a Responsible AI Framework: 5 Key Principles for Organizations](https://professional.dce.harvard.edu/blog/building-a-responsible-ai-framework-5-key-principles-for-organizations/#Building-a-Responsible-AI-Strategy-Best-Practices "This link leaves the DubBlog website.")  
~ Lizzy Short

AI is _everywhere_, moving at lightning speed, bringing incredible opportunities. As we push the boundaries of what AI can do, we need to strike a careful balance with that momentum and a strong commitment to ethical and responsible governance. Innovation shouldn’t come at the cost of our values. That means building with fairness and transparency in mind, [reducing environmental impact](https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117 "This link leaves the DubBlog website."), and making sure these tools are accessible to the broadest range of users possible. Responsible AI isn’t just about what we can build—it’s about building in a way that benefits everyone.

## Resources

-   [What is AI governance](https://www.ibm.com/think/topics/ai-governance "This link leaves the DubBlog website.")?
-   [Responsible AI](https://www.youtube.com/channel/UCQYqGrtjZirXSMlJ5kgrk2w "This link leaves the DubBlog website.") (YouTube series)
-   [Beyond the algorithm: AI’s societal impact](https://mitsloan.mit.edu/ideas-made-to-matter/beyond-algorithm-ais-societal-impact "This link leaves the DubBlog website.")
-   [AI and A11Y: Navigating the AI Revolution](https://dubbot.com/dubblog/2023/ai-and-a11y.html "This link leaves the DubBlog website.")
-   [An Overview Of AI Governance Approaches](https://www.forbes.com/councils/forbestechcouncil/2024/09/30/an-overview-of-ai-governance-approaches/ "This link leaves the DubBlog website.")
-   [How AI Is Impacting Society And Shaping The Future](https://www.forbes.com/sites/kalinabryant/2023/12/13/how-ai-is-impacting-society-and-shaping-the-future/ "This link leaves the DubBlog website.")
-   [The Evolving Landscape Of AI: Responsibility, Accessibility, And Platforms](https://www.forbes.com/councils/forbestechcouncil/2025/05/16/the-evolving-landscape-of-ai-responsibility-accessibility-and-platforms/ "This link leaves the DubBlog website.")
-   [Building a Responsible AI Framework: 5 Key Principles for Organizations](https://professional.dce.harvard.edu/blog/building-a-responsible-ai-framework-5-key-principles-for-organizations/ "This link leaves the DubBlog website.")

A human author creates the DubBlog posts. The AI tool Gemini is sometimes used to brainstorm subject ideas, generate blog post outlines, and rephrase certain portions of the content. Our marketing team carefully reviews all final drafts for accuracy and authenticity. The opinions and perspectives expressed remain the sole responsibility of the human author.